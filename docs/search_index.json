[["regression-and-anova.html", "Chapter 5 Regression and ANOVA 5.1 Your first Ph.D. statistics course is almost done 5.2 What Do We Do Today? 5.3 One-way ANOVA 5.4 Factorial ANOVA 5.5 Regression analysis", " Chapter 5 Regression and ANOVA 5.1 Your first Ph.D. statistics course is almost done Keep up the excellent work! You’re nearly there! 5.1.1 Regression vs. ANOVA In general, remember that both regression and ANOVA are part of the broader class of general linear models. The term ‘general linear model’ refers to conventional linear regression models for a continuous response variable given continuous and/or categorical predictors. For your information, the term ‘generalized linear model’ refers to a larger class of models that is assumed to follow an exponential family distribution. See https://online.stat.psu.edu/stat504/lesson/6/6.1. For your additional information, the term ‘generalized linear mixed model’ refers to an even further extension of general linear models that permis random effects as well as fixed effects in the linear predictor. See https://online.stat.psu.edu/stat504/lesson/generalized-linear-mixed-models. Out of curiosity… are you familiar with ANCOVA…? 5.2 What Do We Do Today? 5.2.1 Goals HW7 (one-way ANOVA) and HW8 (factorial ANOVA) are upcoming. Therefore, we will first discuss ANOVA, focusing on helping you succeed with these assignments. We will also discuss regression analysis. Let’s approach it with a relaxed and review-oriented mindset. 5.2.2 Packages Initial setting # Clean the workspace rm(list=ls());gc() ## used (Mb) gc trigger (Mb) limit (Mb) max used (Mb) ## Ncells 541076 28.9 1211407 64.7 NA 669048 35.8 ## Vcells 967822 7.4 8388608 64.0 16384 1840141 14.1 # Set the working directory setwd(&quot;/Users/ihnwhiheo/Github/PSY202A_Modeling&quot;) getwd() ## [1] &quot;/Users/ihnwhiheo/Github/PSY202A_Modeling&quot; Let’s load R packages that will be used throughout. # List of packages packages &lt;- c(&quot;psych&quot;, &quot;QuantPsyc&quot;, &quot;car&quot;, &quot;apaTables&quot;, &quot;report&quot;, &quot;multcomp&quot;, &quot;palmerpenguins&quot;, &quot;tidyverse&quot;, &quot;ggformula&quot;, &quot;emmeans&quot;) # Add the package names you need # Check, install, and load each package for (package_name in packages) { if (!requireNamespace(package_name, quietly = TRUE)) { install.packages(package_name, dependencies = TRUE) } library(package_name, character.only = TRUE) } ## Warning: package &#39;dplyr&#39; was built under R version 4.2.3 ## Warning: package &#39;ggplot2&#39; was built under R version 4.2.3 ## Warning: package &#39;tidyr&#39; was built under R version 4.2.3 ## Warning: package &#39;readr&#39; was built under R version 4.2.3 ## Warning: package &#39;stringr&#39; was built under R version 4.2.3 ## Warning: package &#39;ggformula&#39; was built under R version 4.2.3 ## Warning: package &#39;scales&#39; was built under R version 4.2.3 ## Warning: package &#39;ggridges&#39; was built under R version 4.2.3 We will be using with the sat.act dataset in the R psych package. The dataset consists of self-reported scores of 700 subjects on the SAT Verbal, SAT Quantitative and ACT. Additional variables include age, gender (0=female, 1=male), and education (). # Load data data(sat.act) dat &lt;- sat.act In the original dataset, gender is coded as 1 for males and 2 for females. The education variable is categorical, ranging from 1 (high school) to 5 (graduate work). For the variables coded as 0, I could not find the exact coding scheme, so we assume that 0 refers to no education. While the other variables are continuous and require no data preprocessing, we will perform some quick data engineering for gender and education. Specifically, we will recode gender so that females (originally coded as 2) become the reference category by assigning them a value of 0. Additionally, we will treat the education variable as categorical. # Data engineering dat$gender_original &lt;- dat$gender dat$gender &lt;- ifelse(dat$gender == 1, 1, 0) dat$education &lt;- as.factor(dat$education) # Quick overview of the data head(dat, 20) ## gender education age ACT SATV SATQ gender_original ## 29442 0 3 19 24 500 500 2 ## 29457 0 3 23 35 600 500 2 ## 29498 0 3 20 21 480 470 2 ## 29503 1 4 27 26 550 520 1 ## 29504 1 2 33 31 600 550 1 ## 29518 1 5 26 28 640 640 1 ## 29527 0 5 30 36 610 500 2 ## 29529 1 3 19 22 520 560 1 ## 29543 0 4 23 22 400 600 2 ## 29547 0 5 40 35 730 800 2 ## 29578 1 3 23 32 760 710 1 ## 29592 0 4 34 29 710 600 2 ## 29617 1 4 32 21 600 600 1 ## 29619 0 4 41 35 780 725 2 ## 29633 0 3 20 27 640 630 2 ## 29671 0 4 24 27 640 590 2 ## 29676 0 3 19 33 640 650 2 ## 29685 0 4 24 32 700 620 2 ## 29710 1 4 35 28 640 580 1 ## 29711 0 4 46 32 610 680 2 Shall we look at the descriptive statistics? # Descriptive statistics describe(dat) ## vars n mean sd median trimmed mad min max range ## gender 1 700 0.35 0.48 0 0.32 0.00 0 1 1 ## education* 2 700 4.16 1.43 4 4.31 1.48 1 6 5 ## age 3 700 25.59 9.50 22 23.86 5.93 13 65 52 ## ACT 4 700 28.55 4.82 29 28.84 4.45 3 36 33 ## SATV 5 700 612.23 112.90 620 619.45 118.61 200 800 600 ## SATQ 6 687 610.22 115.64 620 617.25 118.61 200 800 600 ## gender_original 7 700 1.65 0.48 2 1.68 0.00 1 2 1 ## skew kurtosis se ## gender 0.61 -1.62 0.02 ## education* -0.68 -0.07 0.05 ## age 1.64 2.42 0.36 ## ACT -0.66 0.53 0.18 ## SATV -0.64 0.33 4.27 ## SATQ -0.59 -0.02 4.41 ## gender_original -0.61 -1.62 0.02 For categorical variables (i.e., gender and education), let’s make a table and view the frequencies. # Descriptive statistics tab &lt;- table(Gender = dat$gender, Education = dat$education) addmargins(tab) ## Education ## Gender 0 1 2 3 4 5 Sum ## 0 30 25 21 195 87 95 453 ## 1 27 20 23 80 51 46 247 ## Sum 57 45 44 275 138 141 700 5.3 One-way ANOVA 5.3.1 Assumptions Trailer: You will literally enjoy assumptions in PSY202B with Sarah! Normality: We assume that the populations from which the samples were taken are normally distributed. Numerically, we can check the skewness and kurtosis of the variables. For instance, # Skewness and kurtosis skew(dat$ACT) # Little bit negatively skewed (if 0, no skew; if positive, positively skewed) ## [1] -0.6564026 kurtosi(dat$ACT) # Little bit leptokurtic (if 0, mesokurti;, if negative, platykurtic) ## [1] 0.5349691 Visually, we can check the QQ plot. For instance: # QQ plot qqnorm(dat$ACT) qqline(dat$ACT) Or simply we can check the histogram. # Histogram hist(dat$ACT) Or we can examine the density plot to compare the distribution of the variable against the ideal normal distribution. # Density plot plotNormX(dat$ACT) Random sampling: We assume that the data we measure were obtained from a sample that we selected using a random sampling procedure. We cannot statistically test whether random sampling has occurred. Think about the data collection method you used. Independence: We assume that the probabilities of each measured outcome in a study are independent or equal. For the independence assumption, we cannot test this statistically. Just think about the data collection method you used. Homogeneity of variance (i.e., homoscedasticity): We assume that the samples were drawn from populations of equal variances. Numerically, we can conduct Barlett’s or Levene’s test. If significant, the assumption of homogeneity of variances is violated. # Bartlett&#39;s test bartlett.test(ACT ~ gender, data = dat) ## ## Bartlett test of homogeneity of variances ## ## data: ACT by gender ## Bartlett&#39;s K-squared = 1.9164, df = 1, p-value = 0.1663 bartlett.test(ACT ~ education, data = dat) ## ## Bartlett test of homogeneity of variances ## ## data: ACT by education ## Bartlett&#39;s K-squared = 21.624, df = 5, p-value = 0.0006172 # Levene&#39;s test leveneTest(dat$ACT, group = dat$gender) ## Warning in leveneTest.default(dat$ACT, group = dat$gender): dat$gender coerced ## to factor. ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 0.2899 0.5904 ## 698 leveneTest(dat$ACT, group = dat$education) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 5 3.279 0.006194 ** ## 694 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Visually, we can look at the box plots # Boxplots boxplot(ACT ~ gender, data = dat) boxplot(ACT ~ education, data = dat) 5.3.2 Analysis Let’s conduct a one-way ANOVA to test whether there are significant mean differences in ACT scores across the different education levels. # Perform a one-way ANOVA model_oaov &lt;- aov(ACT ~ education, data = dat) # Print the output summary(model_oaov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## education 5 470 93.90 4.126 0.00106 ** ## Residuals 694 15794 22.76 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Make the APA-style table apa.aov.table(model_oaov) ## ## ## ANOVA results using ACT as the dependent variable ## ## ## Predictor SS df MS F p partial_eta2 CI_90_partial_eta2 ## (Intercept) 43023.79 1 43023.79 1890.50 .000 ## education 469.50 5 93.90 4.13 .001 .03 [.01, .05] ## Error 15793.94 694 22.76 ## ## Note: Values in square brackets indicate the bounds of the 90% confidence interval for partial eta-squared # Describe the result following the APA-style; but for reference ONLY! report(model_oaov) ## The ANOVA (formula: ACT ~ education) suggests that: ## ## - The main effect of education is statistically significant and small (F(5, ## 694) = 4.13, p = 0.001; Eta2 = 0.03, 95% CI [7.33e-03, 1.00]) ## ## Effect sizes were labelled following Field&#39;s (2013) recommendations. 5.3.3 Can you explain to me how those results have been obtained? I’ll leave this as food for thought for you! 5.3.4 How can we interpret the results? According to the ANOVA summary table, the omnibus F-test is statistically significant, as the p-value is below the alpha level of 0.05. This indicates that the null hypothesis of equal population means across the six education groups is rejected. Consequently, we conclude that the six education levels are associated with different ACT scores. 5.3.5 Given the results, we are now interested in testing the differences among all the pairs of the means. Because using multiple t-tests can inflate familywise error rates, we will use one of the correction methods. What should we do? Note that Bonferroni and Holm-Bonferroni are a priori comparison methods. This means they are used when we have a planned set of comparisons before collecting data. In our case, we did not have such a plan (or did you have something in mind? If then, you are amazing!). Therefore, we will use either Fisher’s LSD or Tukey’s HSD for post hoc comparisons. Fisher’s LSD is typically used when there are only three means to compare. This is because, with three means, the family-wise error rate remains controlled at the significance level of alpha. However, when comparing more than three means, the overall F-test does not adequately protect the family-wise error rate if the complete null hypothesis is not true but a subset of it is. Tukey’s HSD, on the other hand, is specifically designed to perform all possible pairwise comparisons. It ensures that the researcher has only a 5% chance of finding a significant mean difference when the null hypothesis is true. In the current example, there are 15 comparisons to be made (Can you understand why? If yes, please explain that to me!). Thus, Tukey’s HSD is the appropriate method among the four correction techniques. # Tukey&#39;s HSD TukeyHSD(model_oaov) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = ACT ~ education, data = dat) ## ## $education ## diff lwr upr p adj ## 1-0 0.01520468 -2.70335124 2.733761 1.0000000 ## 2-0 -0.49641148 -3.23217649 2.239354 0.9954530 ## 3-0 0.82086124 -1.16316374 2.804886 0.8453156 ## 4-0 1.78718535 -0.35927170 3.933642 0.1648941 ## 5-0 2.12915267 -0.01061924 4.268925 0.0520133 ## 2-1 -0.51161616 -3.40192518 2.378693 0.9959541 ## 3-1 0.80565657 -1.38656404 2.997877 0.9005816 ## 4-1 1.77198068 -0.56826588 4.112227 0.2562269 ## 5-1 2.11394799 -0.22016851 4.448064 0.1015015 ## 3-2 1.31727273 -0.89625276 3.530798 0.5317982 ## 4-2 2.28359684 -0.07661879 4.643812 0.0644453 ## 5-2 2.62556415 0.27142658 4.979702 0.0186900 ## 4-3 0.96632411 -0.45584423 2.388492 0.3775325 ## 5-3 1.30829142 -0.10376689 2.720350 0.0874803 ## 5-4 0.34196731 -1.29046383 1.974398 0.9911202 # Test whether the adjusted p-values are lower than 0.05 TukeyHSD(model_oaov)$education[,4] &lt; 0.05 ## 1-0 2-0 3-0 4-0 5-0 2-1 3-1 4-1 5-1 3-2 4-2 5-2 4-3 ## FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## 5-3 5-4 ## FALSE FALSE The adjusted p-values based on Tukey’s HSD indicate a significant difference when comparing the means of group 2 and group 5. Therefore, the group means for 2 and 5 are significantly different. 5.3.6 Something more about a priori contrasts and post-hoc comparison For our valuable learning experiences, we can also use other packages for multiple comparisons. Let’s start with the usage of the multcomp package. The glht function is used to create a test for multiple contrasts. The contrasts can be all-possible pairwise comparisons, which is referred to as “Tukey” in this package. Then, the result of multiple contrasts can be summarized and adjusted for familywise error rate. The Bonferroni method is used in this example. Note that if the test argument is not specified, the single-step approach is used. The adjusted p values is computed from the joint normal or t distribution of the z statistics such that the p value represents the probability of getting at least one significant result by chance if all z or t values are the same in all contrasts. The Tukey method and the single-step approach will provide the same results if the group sizes are equal. # Multiple comparisons pairwise &lt;- glht(model_oaov, linfct = mcp(education = &quot;Tukey&quot;)) summary(pairwise, test = adjusted(type = &quot;bonferroni&quot;)) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: aov(formula = ACT ~ education, data = dat) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## 1 - 0 == 0 0.0152 0.9513 0.016 1.0000 ## 2 - 0 == 0 -0.4964 0.9573 -0.519 1.0000 ## 3 - 0 == 0 0.8209 0.6943 1.182 1.0000 ## 4 - 0 == 0 1.7872 0.7511 2.379 0.2642 ## 5 - 0 == 0 2.1292 0.7488 2.844 0.0689 . ## 2 - 1 == 0 -0.5116 1.0114 -0.506 1.0000 ## 3 - 1 == 0 0.8057 0.7671 1.050 1.0000 ## 4 - 1 == 0 1.7720 0.8189 2.164 0.4623 ## 5 - 1 == 0 2.1139 0.8168 2.588 0.1478 ## 3 - 2 == 0 1.3173 0.7746 1.701 1.0000 ## 4 - 2 == 0 2.2836 0.8259 2.765 0.0877 . ## 5 - 2 == 0 2.6256 0.8238 3.187 0.0225 * ## 4 - 3 == 0 0.9663 0.4977 1.942 0.7886 ## 5 - 3 == 0 1.3083 0.4941 2.648 0.1243 ## 5 - 4 == 0 0.3420 0.5712 0.599 1.0000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- bonferroni method) The simultaneous confidence interval can be computed by the confint function: # Confidence interval confint(pairwise) ## ## Simultaneous Confidence Intervals ## ## Multiple Comparisons of Means: Tukey Contrasts ## ## ## Fit: aov(formula = ACT ~ education, data = dat) ## ## Quantile = 2.829 ## 95% family-wise confidence level ## ## ## Linear Hypotheses: ## Estimate lwr upr ## 1 - 0 == 0 0.01520 -2.67606 2.70647 ## 2 - 0 == 0 -0.49641 -3.20472 2.21189 ## 3 - 0 == 0 0.82086 -1.14325 2.78497 ## 4 - 0 == 0 1.78719 -0.33773 3.91210 ## 5 - 0 == 0 2.12915 0.01086 4.24745 ## 2 - 1 == 0 -0.51162 -3.37291 2.34968 ## 3 - 1 == 0 0.80566 -1.36456 2.97587 ## 4 - 1 == 0 1.77198 -0.54478 4.08874 ## 5 - 1 == 0 2.11395 -0.19674 4.42464 ## 3 - 2 == 0 1.31727 -0.87403 3.50858 ## 4 - 2 == 0 2.28360 -0.05293 4.62012 ## 5 - 2 == 0 2.62556 0.29506 4.95607 ## 4 - 3 == 0 0.96632 -0.44157 2.37422 ## 5 - 3 == 0 1.30829 -0.08959 2.70618 ## 5 - 4 == 0 0.34197 -1.27408 1.95801 plot(confint(pairwise)) Instead of post hoc comparison, researchers may have a priori contrasts from their research hypotheses. For example, researchers expect a linear trend in the impact of education on ACT score. The contrast coefficient of the linear trend for six groups can be created: # Contrast ctr &lt;- matrix(c(-2.5, -1.5, -0.5, 0.5, 1.5, 2.5), 1, 6) The polynomial contrast coefficient is based on Table A10 in Maxwell and Delaney (2004). The contrast is specified in a matrix where rows represent different contrasts and columns represent the coefficient of each group. The contrast can be evaluated using the glht function from the multcomp package: # Simultanoues tests for general linear hypotheses linear &lt;- glht(model_oaov, linfct = mcp(education = ctr)) summary(linear) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: User-defined Contrasts ## ## ## Fit: aov(formula = ACT ~ education, data = dat) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## 1 == 0 8.639 2.272 3.802 0.000156 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- single-step method) The contrast is provided in the linfct argument. Because only one contrast is tested, the familywise error rate correction is not needed. Next, all polynomial contrasts up to the fifth order are investigated for the difference in ACT between education levels: # Different contrasts as examples linear &lt;- c(-5, -3, -1, 1, 3, 5) quadratic &lt;- c(5, -1, -4, -4, -1, 5) cubic &lt;- c(-5, 7, 4, -4, -7, 5) quartic &lt;- c(1, -3, 2, 2, -3, 1) quintic &lt;- c(-1, 5, -10, 10, -5, 1) mctr &lt;- rbind(linear, quadratic, cubic, quartic, quintic) As mentioned above, the row of the contrast matrix represents different contrasts. The matrix of multiple contrasts can be used in the glht function: # Multiple testing polynomial &lt;- glht(model_oaov, linfct = mcp(education = mctr)) summary(polynomial, test=adjusted(type = &quot;bonferroni&quot;)) ## ## Simultaneous Tests for General Linear Hypotheses ## ## Multiple Comparisons of Means: User-defined Contrasts ## ## ## Fit: aov(formula = ACT ~ education, data = dat) ## ## Linear Hypotheses: ## Estimate Std. Error t value Pr(&gt;|t|) ## linear == 0 17.279 4.544 3.802 0.00078 *** ## quadratic == 0 7.546 4.928 1.531 0.63099 ## cubic == 0 -7.027 7.515 -0.935 1.00000 ## quartic == 0 -2.629 2.999 -0.877 1.00000 ## quintic == 0 6.442 8.793 0.733 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## (Adjusted p values reported -- bonferroni method) 5.4 Factorial ANOVA Shall we use a different dataset this time? Here, assume we are interested in testing whether there are significant mean differences in body mass across the different species and sex levels. There are three different types for the species variable and two levels for the sex variable. # Use penguins data dat &lt;- penguins # Quick look at the data str(dat) ## tibble [344 × 8] (S3: tbl_df/tbl/data.frame) ## $ species : Factor w/ 3 levels &quot;Adelie&quot;,&quot;Chinstrap&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ island : Factor w/ 3 levels &quot;Biscoe&quot;,&quot;Dream&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ bill_length_mm : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ... ## $ bill_depth_mm : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ... ## $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ... ## $ body_mass_g : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 1 NA 1 2 1 2 NA NA ... ## $ year : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ... describe(dat) ## vars n mean sd median trimmed mad min max ## species* 1 344 1.92 0.89 2.00 1.90 1.48 1.0 3.0 ## island* 2 344 1.66 0.73 2.00 1.58 1.48 1.0 3.0 ## bill_length_mm 3 342 43.92 5.46 44.45 43.91 7.04 32.1 59.6 ## bill_depth_mm 4 342 17.15 1.97 17.30 17.17 2.22 13.1 21.5 ## flipper_length_mm 5 342 200.92 14.06 197.00 200.34 16.31 172.0 231.0 ## body_mass_g 6 342 4201.75 801.95 4050.00 4154.01 889.56 2700.0 6300.0 ## sex* 7 333 1.50 0.50 2.00 1.51 0.00 1.0 2.0 ## year 8 344 2008.03 0.82 2008.00 2008.04 1.48 2007.0 2009.0 ## range skew kurtosis se ## species* 2.0 0.16 -1.73 0.05 ## island* 2.0 0.61 -0.91 0.04 ## bill_length_mm 27.5 0.05 -0.89 0.30 ## bill_depth_mm 8.4 -0.14 -0.92 0.11 ## flipper_length_mm 59.0 0.34 -1.00 0.76 ## body_mass_g 3600.0 0.47 -0.74 43.36 ## sex* 1.0 -0.02 -2.01 0.03 ## year 2.0 -0.05 -1.51 0.04 summary(dat) ## species island bill_length_mm bill_depth_mm ## Adelie :152 Biscoe :168 Min. :32.10 Min. :13.10 ## Chinstrap: 68 Dream :124 1st Qu.:39.23 1st Qu.:15.60 ## Gentoo :124 Torgersen: 52 Median :44.45 Median :17.30 ## Mean :43.92 Mean :17.15 ## 3rd Qu.:48.50 3rd Qu.:18.70 ## Max. :59.60 Max. :21.50 ## NA&#39;s :2 NA&#39;s :2 ## flipper_length_mm body_mass_g sex year ## Min. :172.0 Min. :2700 female:165 Min. :2007 ## 1st Qu.:190.0 1st Qu.:3550 male :168 1st Qu.:2007 ## Median :197.0 Median :4050 NA&#39;s : 11 Median :2008 ## Mean :200.9 Mean :4202 Mean :2008 ## 3rd Qu.:213.0 3rd Qu.:4750 3rd Qu.:2009 ## Max. :231.0 Max. :6300 Max. :2009 ## NA&#39;s :2 NA&#39;s :2 For the ease of interpretation and implementation, I excluded penguins where the sex variable is missing: # Data engineering for exclusing cases where sex variable is missing dat &lt;- dat %&gt;% filter(!is.na(sex)) # Quick look at the modified data str(dat) ## tibble [333 × 8] (S3: tbl_df/tbl/data.frame) ## $ species : Factor w/ 3 levels &quot;Adelie&quot;,&quot;Chinstrap&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ island : Factor w/ 3 levels &quot;Biscoe&quot;,&quot;Dream&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ bill_length_mm : num [1:333] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 41.1 38.6 34.6 ... ## $ bill_depth_mm : num [1:333] 18.7 17.4 18 19.3 20.6 17.8 19.6 17.6 21.2 21.1 ... ## $ flipper_length_mm: int [1:333] 181 186 195 193 190 181 195 182 191 198 ... ## $ body_mass_g : int [1:333] 3750 3800 3250 3450 3650 3625 4675 3200 3800 4400 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 1 1 2 1 2 1 2 2 ... ## $ year : int [1:333] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ... describe(dat) ## vars n mean sd median trimmed mad min max ## species* 1 333 1.92 0.89 2.0 1.90 1.48 1.0 3.0 ## island* 2 333 1.65 0.71 2.0 1.57 1.48 1.0 3.0 ## bill_length_mm 3 333 43.99 5.47 44.5 43.98 6.97 32.1 59.6 ## bill_depth_mm 4 333 17.16 1.97 17.3 17.19 2.22 13.1 21.5 ## flipper_length_mm 5 333 200.97 14.02 197.0 200.36 16.31 172.0 231.0 ## body_mass_g 6 333 4207.06 805.22 4050.0 4159.46 889.56 2700.0 6300.0 ## sex* 7 333 1.50 0.50 2.0 1.51 0.00 1.0 2.0 ## year 8 333 2008.04 0.81 2008.0 2008.05 1.48 2007.0 2009.0 ## range skew kurtosis se ## species* 2.0 0.16 -1.72 0.05 ## island* 2.0 0.62 -0.85 0.04 ## bill_length_mm 27.5 0.04 -0.90 0.30 ## bill_depth_mm 8.4 -0.15 -0.91 0.11 ## flipper_length_mm 59.0 0.36 -0.98 0.77 ## body_mass_g 3600.0 0.47 -0.75 44.13 ## sex* 1.0 -0.02 -2.01 0.03 ## year 2.0 -0.08 -1.49 0.04 summary(dat) ## species island bill_length_mm bill_depth_mm ## Adelie :146 Biscoe :163 Min. :32.10 Min. :13.10 ## Chinstrap: 68 Dream :123 1st Qu.:39.50 1st Qu.:15.60 ## Gentoo :119 Torgersen: 47 Median :44.50 Median :17.30 ## Mean :43.99 Mean :17.16 ## 3rd Qu.:48.60 3rd Qu.:18.70 ## Max. :59.60 Max. :21.50 ## flipper_length_mm body_mass_g sex year ## Min. :172 Min. :2700 female:165 Min. :2007 ## 1st Qu.:190 1st Qu.:3550 male :168 1st Qu.:2007 ## Median :197 Median :4050 Median :2008 ## Mean :201 Mean :4207 Mean :2008 ## 3rd Qu.:213 3rd Qu.:4775 3rd Qu.:2009 ## Max. :231 Max. :6300 Max. :2009 5.4.1 How can we characterize this design? There are two factors in this study. The first factor is species, and the second factor is sex. Species has three levels (Adelie, Chinstrap, and Gentoo), and sex has two levels (female and male). In total, there are six combinations formed by the three levels of species and the two levels of sex, with penguins assigned to all six combinations. This indicates that the current study employs a \\(3 \\times 2\\) factorial design. For this design, a two-way ANOVA should be conducted. The three null hypotheses being tested are as follows: \\(H_0\\) (regarding species): Species does not have a significant main effect on body mass (group means on body mass across the three levels of species are not different). \\(H_0\\) (regarding sex): Sex does not have a significant main effect on body mass (group means on body mass across the two levels of sex are not different). \\(H_0\\) (regarding interaction): There is no significant interaction between species and sex on body mass. 5.4.2 Cells means for the data # Mean by groups in a wide format dat.tab &lt;- dat %&gt;% group_by(species, sex) %&gt;% summarise(mean_bodymass = mean(body_mass_g)) %&gt;% spread(sex, mean_bodymass) ## `summarise()` has grouped output by &#39;species&#39;. You can override using the ## `.groups` argument. # Add the marginal means for species dat.tab$Means &lt;- rowMeans(dat.tab[2:3]) # Add the marginal means for sex col_means &lt;- c(colMeans(dat.tab[2:4])) dat.tab &lt;- rbind(dat.tab, col_means) dat.tab$species &lt;- c(&quot;Adelie&quot;, &quot;Chinstrap&quot;, &quot;Gentoo&quot;, &quot;Means&quot;) # Print the table dat.tab ## # A tibble: 4 × 4 ## # Groups: species [4] ## species female male Means ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Adelie 3369. 4043. 3706. ## 2 Chinstrap 3527. 3939. 3733. ## 3 Gentoo 4680. 5485. 5082. ## 4 Means 3859. 4489. 4174. Can you understand the means by groups? In particular: Can you find any simple effect? (i.e., the effect of one IV at a specific level of the other IV) Can you find any main effect? (i.e., the average effect of the IV across the levels of all other IVs) Can you find any interaction effect? (i.e., the simple effects of one IV are not constant across all levels of the other IV; there is a differential effect) 5.4.3 Visualizing cell means # Jittered scatterplot with mean and SE dat %&gt;% ggplot(aes(x = species, y = body_mass_g)) + facet_wrap(~sex) + stat_summary(fun.dat = &quot;mean_se&quot;, geom = &quot;errorbar&quot;, wide = 0.5) + stat_summary(fun.dat = &quot;mean_se&quot;, geom = &quot;pointrange&quot;) + geom_jitter(cex = 1.5, pch = 1.0) ## Warning in stat_summary(fun.dat = &quot;mean_se&quot;, geom = &quot;errorbar&quot;, wide = 0.5): ## Ignoring unknown parameters: `fun.dat` and `wide` ## Warning in stat_summary(fun.dat = &quot;mean_se&quot;, geom = &quot;pointrange&quot;): Ignoring ## unknown parameters: `fun.dat` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` ## No summary function supplied, defaulting to `mean_se()` What can you observe from the plot? 5.4.4 Analysis Let’s conduct a two-way ANOVA to test whether there are mean differences in body mass across the different species and sex levels. # Perform a two-way ANOVA # Both of the code below do the same job model_taov &lt;- aov(body_mass_g ~ species + sex + species:sex, data = dat) ## The colon represents the interaction effect only model_taov &lt;- aov(body_mass_g ~ species*sex, data = dat) ## The asterisk represents the interaction effect and the main effect of each variable ## (and all lower-order interactions) # Results summary(model_taov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## species 2 145190219 72595110 758.358 &lt; 2e-16 *** ## sex 1 37090262 37090262 387.460 &lt; 2e-16 *** ## species:sex 2 1676557 838278 8.757 0.000197 *** ## Residuals 327 31302628 95727 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Make the APA-style table apa.aov.table(model_taov) ## ## ## ANOVA results using body_mass_g as the dependent variable ## ## ## Predictor SS df MS F p partial_eta2 ## (Intercept) 828480898.97 1 828480898.97 8654.65 .000 ## species 60350016.02 2 30175008.01 315.22 .000 .66 ## sex 16613441.78 1 16613441.78 173.55 .000 .35 ## species x sex 1676556.74 2 838278.37 8.76 .000 .05 ## Error 31302628.28 327 95726.69 ## CI_90_partial_eta2 ## ## [.61, .69] ## [.28, .41] ## [.02, .09] ## ## ## Note: Values in square brackets indicate the bounds of the 90% confidence interval for partial eta-squared # Describe the result following the APA-style; but for reference ONLY! report(model_taov) ## The ANOVA (formula: body_mass_g ~ species * sex) suggests that: ## ## - The main effect of species is statistically significant and large (F(2, 327) ## = 758.36, p &lt; .001; Eta2 (partial) = 0.82, 95% CI [0.80, 1.00]) ## - The main effect of sex is statistically significant and large (F(1, 327) = ## 387.46, p &lt; .001; Eta2 (partial) = 0.54, 95% CI [0.49, 1.00]) ## - The interaction between species and sex is statistically significant and ## small (F(2, 327) = 8.76, p &lt; .001; Eta2 (partial) = 0.05, 95% CI [0.02, 1.00]) ## ## Effect sizes were labelled following Field&#39;s (2013) recommendations. 5.4.5 Can you explain to me how those results have been obtained? I’ll leave this as food for thought for you! 5.4.6 How can we interpret the results? Let’s use an alpha level of 0.05 as the significance threshold. According to the results, the p-value for species is lower than the significance level, indicating that species has a significant effect on body mass. In other words, the group means for body mass differ significantly across the three species. The p-value for sex is also below the alpha level of 0.05, suggesting that sex has a significant effect on body mass. In other words, there are differences in body mass between female and male penguins. Lastly, the p-value for the interaction between species and sex (denoted as species:sex in the output) is below the significance threshold. This indicates a significant interaction between species and sex. That is, the effect of species on body mass is not consistent across the levels of sex, as observed in the plot above. 5.4.7 What to do with the interaction effect? The interaction in the analysis suggests that it would be beneficial to examine the simple effects. The next step is to analyze the differences in body mass due to species within each level of sex. To do this, we will split the dataset and conduct a separate analysis for each level of sex. Importantly, to do this, we use the mean-squared-within (i.e., \\(\\text{MS}_{\\text{within}}\\)) from the full data to compute the F value, and then use the pf function to find the p-value. To control the overall Type I error rate, we will use an alpha level of 0.025 for each test. 5.4.7.1 What are you talking about, Ihnwhi…? Let’s proceed with the analyses! # Split data ## female dat_f &lt;- filter(dat, sex == &quot;female&quot;) model_taov_f &lt;- aov(body_mass_g ~ species, data = dat_f) summary(model_taov_f) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## species 2 60350016 30175008 393.2 &lt;2e-16 *** ## Residuals 162 12430757 76733 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # MS_between from the subset of data MS_b_f &lt;- summary(model_taov_f)[[1]][1,3] # df_between frmo the subset of data df_b_f &lt;- summary(model_taov_f)[[1]][1,1] # MS_within from the full data MS_w &lt;- summary(model_taov)[[1]][4,3] # df_within from the full data df_w &lt;- summary(model_taov)[[1]][4,1] # Calculate the F-value F_f &lt;- MS_b_f/MS_w # Obtain the p-value p_f &lt;- pf(F_f, df1 = df_b_f, df2 = df_w, lower.tail = FALSE) # Make a statistical decision - Is p-value lower than the adjusted alpha? p_f &lt; 0.025 ## [1] TRUE The results indicate that the p-value is lower than the adjusted alpha level of 0.025 based on Bonferroni’s correction. This suggests that there are significant mean differences across the three levels of species when the level of sex is female. We do the same for the subset of the data where the level of sex is male. # Split data ## male dat_m &lt;- filter(dat, sex == &quot;male&quot;) model_taov_m &lt;- aov(body_mass_g ~ species, data = dat_m) summary(model_taov_m) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## species 2 84728125 42364062 370.4 &lt;2e-16 *** ## Residuals 165 18871872 114375 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # MS_between from the subset of data MS_b_m &lt;- summary(model_taov_m)[[1]][1,3] # df_between frmo the subset of data df_b_m &lt;- summary(model_taov_m)[[1]][1,1] # MS_within from the full data MS_w &lt;- summary(model_taov)[[1]][4,3] # df_within from the full data df_w &lt;- summary(model_taov)[[1]][4,1] # Calculate the F-value F_m &lt;- MS_b_m/MS_w # Obtain the p-value p_m &lt;- pf(F_m, df1 = df_b_m, df2 = df_w, lower.tail = FALSE) # Make a statistical decision - Is p-value lower than the adjusted alpha? p_m &lt; 0.025 ## [1] TRUE Similarly, the p-value is lower than the adjusted alpha level of 0.025 based on Bonferroni’s correction. This indicates that there are significant mean differences across the three levels of species when the level of sex is male. 5.4.8 This time, let’s use pairwise comparison with Tukey’s HSD to elaborate on the results of F. We will use the lsmeans() function to perform pairwise comparisons based on Tukey’s HSD. # Pairwise comparison lsmeans(model_taov, pairwise ~ species, by = &quot;sex&quot;, adjust = &quot;tukey&quot;) ## $lsmeans ## sex = female: ## species lsmean SE df lower.CL upper.CL ## Adelie 3369 36.2 327 3298 3440 ## Chinstrap 3527 53.1 327 3423 3632 ## Gentoo 4680 40.6 327 4600 4760 ## ## sex = male: ## species lsmean SE df lower.CL upper.CL ## Adelie 4043 36.2 327 3972 4115 ## Chinstrap 3939 53.1 327 3835 4043 ## Gentoo 5485 39.6 327 5407 5563 ## ## Confidence level used: 0.95 ## ## $contrasts ## sex = female: ## contrast estimate SE df t.ratio p.value ## Adelie - Chinstrap -158 64.2 327 -2.465 0.0377 ## Adelie - Gentoo -1311 54.4 327 -24.088 &lt;.0001 ## Chinstrap - Gentoo -1153 66.8 327 -17.246 &lt;.0001 ## ## sex = male: ## contrast estimate SE df t.ratio p.value ## Adelie - Chinstrap 105 64.2 327 1.627 0.2357 ## Adelie - Gentoo -1441 53.7 327 -26.855 &lt;.0001 ## Chinstrap - Gentoo -1546 66.2 327 -23.345 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 3 estimates Can you understand the results? Focus on the $contrasts section! Can you expand on your results? Feel free to refer to the lecture slides for a template to guide your write-up. 5.5 Regression analysis Purpose of Regression Analysis: Regression analysis is a statistical method used to examine the relationship between a dependent variable (outcome) and one or more independent variables (predictors). It helps quantify the strength and direction of these relationships. Key Objective: The primary goal is to model the relationship between variables, allowing researchers to explain variations in the dependent variable and make predictions about future or unobserved data points. Types of Regression: Regression analysis comes in various forms, such as simple regression (one predictor), multiple regression (multiple predictors), and specialized models (e.g., logistic regression for binary outcomes or polynomial regression for nonlinear relationships). Assumptions and Applications: Regression models rely on assumptions such as linearity, independence, homoscedasticity, and normality of residuals. It is widely applied in fields like psychology, economics, and engineering to analyze trends, test hypotheses, and predict outcomes. You will have a deeper understanding into the assumptions in PSY202B with Sarah! Tips When Programming: Be sure to differentiate what is regression on others! 5.5.1 Simple regression analysis A simple regression model is used to describe the relationship between two variables: an independent variable (IV) and a dependent variable (DV). The research question addressed by simple regression is whether the IV has an effect on the DV. If a significant effect is found, the model can be used to predict DV values for new IV values. Assuming a linear relationship between the two variables, the mathematical expression for a simple regression model is \\(y_i = b_0 + b_1x_i + \\varepsilon_i\\), where \\(b_0\\) and \\(b_1\\) are the regression coefficients (intercept and slope, respectively), and \\(\\varepsilon_i\\) represents the error term. The DV must be a continuous variable, while the IV can be either continuous or categorical. For example, one-way ANOVA is a special case of simple regression, where the IV is categorical. 5.5.1.1 Simple regression analysis with a continuous predictor Say our goal is to predict the ACT score using age through a simple regression analysis. What would be the null hypothesis? # Use the sat-act data again dat_satact &lt;- sat.act # Simple regression analysis, with a continuous predictor model_reg_s_cont &lt;- lm(ACT ~ age, data = dat_satact) summary(model_reg_s_cont) ## ## Call: ## lm(formula = ACT ~ age, data = dat_satact) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.3454 -3.1874 0.4301 3.6546 7.9915 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.11035 0.52148 51.988 &lt; 2e-16 *** ## age 0.05614 0.01910 2.939 0.00341 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.797 on 698 degrees of freedom ## Multiple R-squared: 0.01222, Adjusted R-squared: 0.01081 ## F-statistic: 8.635 on 1 and 698 DF, p-value: 0.003406 Can you understand the output and interpret the result? 5.5.1.2 Simple regression analysis with a categorical predictor This time, say we would like to predict the ACT score using education through a simple regression analysis. # Simple regression analysis, with a continuous predictor model_reg_s_cate &lt;- lm(ACT ~ as.factor(education), data = dat_satact) summary(model_reg_s_cate) ## ## Call: ## lm(formula = ACT ~ as.factor(education), data = dat_satact) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.9773 -3.2945 0.5263 3.7055 9.0227 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.4737 0.6319 43.480 &lt; 2e-16 *** ## as.factor(education)1 0.0152 0.9513 0.016 0.98725 ## as.factor(education)2 -0.4964 0.9573 -0.519 0.60425 ## as.factor(education)3 0.8209 0.6943 1.182 0.23748 ## as.factor(education)4 1.7872 0.7511 2.379 0.01761 * ## as.factor(education)5 2.1292 0.7488 2.844 0.00459 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.771 on 694 degrees of freedom ## Multiple R-squared: 0.02887, Adjusted R-squared: 0.02187 ## F-statistic: 4.126 on 5 and 694 DF, p-value: 0.001063 Can you notice something from the above results compared to what we did as below? # Perform a one-way ANOVA model_oaov &lt;- aov(ACT ~ education, data = dat_satact) # Print the output summary(model_oaov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## education 1 390 389.9 17.14 3.89e-05 *** ## Residuals 698 15874 22.7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.5.2 Multiple regression analysis Multiple linear regression allows more than one IVs to predict the dependent variable (DV). To examine effects of multiple IVs on the DV, we can simply add IVs in the linear model, \\(y_i = b_0 + b_1 x_{1i} + b_2 x_{2i} + \\cdots + b_p x_{pi} + \\varepsilon_i\\). 5.5.2.1 Multiple regression analysis with two continuous variables Say our goal is to predict the body mass of penguins using flipper length and bill depth through a multiple regression analysis. What would be the null hypothesis? # Load data again dat_penguin &lt;- penguins # Multiple regression analysis model_reg_m_1 &lt;- lm(body_mass_g ~ flipper_length_mm + bill_depth_mm, data = dat_penguin) summary(model_reg_m_1) ## ## Call: ## lm(formula = body_mass_g ~ flipper_length_mm + bill_depth_mm, ## data = dat_penguin) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1029.78 -271.45 -23.58 245.15 1275.97 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6541.907 540.751 -12.098 &lt;2e-16 *** ## flipper_length_mm 51.541 1.865 27.635 &lt;2e-16 *** ## bill_depth_mm 22.634 13.280 1.704 0.0892 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 393.2 on 339 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.761, Adjusted R-squared: 0.7596 ## F-statistic: 539.8 on 2 and 339 DF, p-value: &lt; 2.2e-16 Can you understand the output and interpret the result? 5.5.2.2 Model diagnostics for multiple regression analysis Trailer: Again, much more in Sarah’s PSY202B! For the purpose of diagnosing model assumptions, we can check four diagnostic plots: (1) residuals vs. fitted, (2) normal Q-Q, (3) scale-loation, (4) Cook’s distance, and (5) residuals vs. leverage. # Model diagnostics using visual tools plot(model_reg_m_1, which = 1) A visual examination of the residuals vs. fitted plot indicates that there is equal variance along the line of best fit (i.e., regression line). Therefore, the assumption of homoscedasticity is met. In addition, the points are randomly scattered across the place, and there are no discernible nonlinear trends. This means that the assumption of linearity is also met. # Model diagnostics using visual tools plot(model_reg_m_1, which = 2) The Q-Q plot shows that there is a slight deviation in the lower-left corner, but this is not severe and looks okay. Also, most of the data points hover around the normal line. Thus, the normality assumption is met. # Model diagnostics using visual tools plot(model_reg_m_1, which = 3) The scale-location plot tells us whether the assumption of homoscedasticity is satisfied or violated. If a model satisfies this assumption, the points should ideally be equally spread around the red horizontal line. According to the plot, the red horizontal line is fairly flat, which suggests that the assumption of homoscedasticity is met. While observations such as 40, 166, and 170 deviate slightly more, I do not believe this indicates a severe violation of the homoscedasticity assumption. # Model diagnostics using visual tools par(mfrow=c(1,2)) plot(model_reg_m_1, which = 4) plot(model_reg_m_1, which = 5) par(mfrow=c(1,1)) To diagnose the existence of influential points that can alter the results of the regression analysis upon the inclusion or exclusion of a value, I checked the Cook’s distance plot and the residuals vs. leverage plot. Cook’s distance measures the influence of each data point on the fitted model. Larger values of Cook’s distance, such as those greater than 0.5, indicate more influential points. According to the Cook’s distance plot, values from observations 40, 170, 166 could be influential, so those values are worthy of closer scrutiny. I checked the residuals vs. leverage plot. According to this plot, observations with standardized residuals greater than 3 in absolute value are possible outliers. Indeed, there were three values whose absolute standardized residuals exceeded 3 (values from observations 40, 166, and 170), indicating the potential presence of outliers. In sum, the assumption of no influential points is likely violated, at least for three specific observations. These values should be closely examined in the substantive research context. 5.5.2.3 Multiple regression analysis with one continuous variable and one categorical variable Say our goal is to predict the body mass of penguins using flipper length and sex through a multiple regression analysis. We use female as a reference category (i.e., coded as 0), where male is coded as 1. What would be the null hypothesis? # Quick data engineering dat_penguin &lt;- dat_penguin %&gt;% mutate(sex_dummy = if_else(sex == &quot;male&quot;, 1, 0)) # Multiple regression analysis model_reg_m_2 &lt;- lm(body_mass_g ~ flipper_length_mm + sex_dummy, data = dat_penguin) summary(model_reg_m_2) ## ## Call: ## lm(formula = body_mass_g ~ flipper_length_mm + sex_dummy, data = dat_penguin) ## ## Residuals: ## Min 1Q Median 3Q Max ## -910.28 -243.89 -2.94 238.85 1067.73 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5410.300 285.798 -18.931 &lt; 2e-16 *** ## flipper_length_mm 46.982 1.441 32.598 &lt; 2e-16 *** ## sex_dummy 347.850 40.342 8.623 2.78e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 355.9 on 330 degrees of freedom ## (11 observations deleted due to missingness) ## Multiple R-squared: 0.8058, Adjusted R-squared: 0.8047 ## F-statistic: 684.8 on 2 and 330 DF, p-value: &lt; 2.2e-16 Can you understand the output and interpret the result? "]]
